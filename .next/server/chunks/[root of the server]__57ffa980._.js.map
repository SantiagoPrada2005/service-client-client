{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 6, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}},
    {"offset": {"line": 60, "column": 0}, "map": {"version":3,"sources":["file:///Users/santiagopradamoreno/Documents/ServiceClient/service-client-client/src/app/utils/AI/Gemini/controller.ts"],"sourcesContent":["import { NextRequest, NextResponse} from 'next/server';\nimport { GoogleGenerativeAI } from '@google/generative-ai';\n\ninterface PromptRequest {\n  prompt: string;\n}\n\n\n// Define proper types for Gemini API\ninterface GeminiChatMessage {\n  role: string;\n  parts: { text: string }[];\n}\n\ntype ConversationEntry = [string, string]; // [userPrompt, modelResponse]\n\n// GoogleGenerativeAI required config\nconst configuration = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || \"\");\n\nif (!process.env.GEMINI_API_KEY) {\n  console.warn(\"Warning: API_KEY environment variable is not set\");\n}\n\n// Model initialization\nconst modelId = \"gemini-2.0-flash\";\nconst model = configuration.getGenerativeModel({ model: modelId });\n\n//These arrays are to maintain the history of the conversation\nconst conversationContext: ConversationEntry[] = [[\"Me llamo Santiago\", \"Genial.\"]];\n\n// Controller function to handle chat conversation\nexport const generateResponse = async (req: NextRequest) => {\n  try {\n    // Parse the request body as JSON\n    const body = await req.json() as PromptRequest;\n    const { prompt } = body;\n    \n    if (!prompt) {\n      return NextResponse.json({ error: \"Prompt is required\" }, { status: 400 });\n    }\n\n    // Build chat history in the format Gemini expects\n    const chatHistory: GeminiChatMessage[] = [];\n    \n    // Restore the previous context\n    for (const [inputText, responseText] of conversationContext) {\n      chatHistory.push({ \n        role: \"user\", \n        parts: [{ text: inputText }]\n      });\n      chatHistory.push({ \n        role: \"model\", \n        parts: [{ text: responseText }]\n      });\n    }\n\n    const chat = model.startChat({\n      history: chatHistory,\n      generationConfig: {\n        maxOutputTokens: 100,\n      },\n    });\n\n    const result = await chat.sendMessage(prompt);\n    \n    const response = await result.response;\n    const responseText = response.text();\n\n    // Stores the conversation\n    conversationContext.push([prompt, responseText]);\n    return NextResponse.json({ response: responseText });\n\n  } catch (err) {\n    console.error(err);\n    return NextResponse.json({ \n      error: \"Internal server error\", \n      details: err instanceof Error ? err.message : String(err) \n    }, { status: 500 });\n  }\n};"],"names":[],"mappings":";;;AAAA;AACA;;;AAeA,qCAAqC;AACrC,MAAM,gBAAgB,IAAI,gKAAA,CAAA,qBAAkB,CAAC,QAAQ,GAAG,CAAC,cAAc,IAAI;AAE3E,IAAI,CAAC,QAAQ,GAAG,CAAC,cAAc,EAAE;IAC/B,QAAQ,IAAI,CAAC;AACf;AAEA,uBAAuB;AACvB,MAAM,UAAU;AAChB,MAAM,QAAQ,cAAc,kBAAkB,CAAC;IAAE,OAAO;AAAQ;AAEhE,8DAA8D;AAC9D,MAAM,sBAA2C;IAAC;QAAC;QAAqB;KAAU;CAAC;AAG5E,MAAM,mBAAmB,OAAO;IACrC,IAAI;QACF,iCAAiC;QACjC,MAAM,OAAO,MAAM,IAAI,IAAI;QAC3B,MAAM,EAAE,MAAM,EAAE,GAAG;QAEnB,IAAI,CAAC,QAAQ;YACX,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CAAC;gBAAE,OAAO;YAAqB,GAAG;gBAAE,QAAQ;YAAI;QAC1E;QAEA,kDAAkD;QAClD,MAAM,cAAmC,EAAE;QAE3C,+BAA+B;QAC/B,KAAK,MAAM,CAAC,WAAW,aAAa,IAAI,oBAAqB;YAC3D,YAAY,IAAI,CAAC;gBACf,MAAM;gBACN,OAAO;oBAAC;wBAAE,MAAM;oBAAU;iBAAE;YAC9B;YACA,YAAY,IAAI,CAAC;gBACf,MAAM;gBACN,OAAO;oBAAC;wBAAE,MAAM;oBAAa;iBAAE;YACjC;QACF;QAEA,MAAM,OAAO,MAAM,SAAS,CAAC;YAC3B,SAAS;YACT,kBAAkB;gBAChB,iBAAiB;YACnB;QACF;QAEA,MAAM,SAAS,MAAM,KAAK,WAAW,CAAC;QAEtC,MAAM,WAAW,MAAM,OAAO,QAAQ;QACtC,MAAM,eAAe,SAAS,IAAI;QAElC,0BAA0B;QAC1B,oBAAoB,IAAI,CAAC;YAAC;YAAQ;SAAa;QAC/C,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CAAC;YAAE,UAAU;QAAa;IAEpD,EAAE,OAAO,KAAK;QACZ,QAAQ,KAAK,CAAC;QACd,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CAAC;YACvB,OAAO;YACP,SAAS,eAAe,QAAQ,IAAI,OAAO,GAAG,OAAO;QACvD,GAAG;YAAE,QAAQ;QAAI;IACnB;AACF","debugId":null}},
    {"offset": {"line": 150, "column": 0}, "map": {"version":3,"sources":["file:///Users/santiagopradamoreno/Documents/ServiceClient/service-client-client/src/app/api/AIHandler/geminiHandler/route.ts"],"sourcesContent":["import { NextRequest } from 'next/server';\nimport { generateResponse } from '../../../utils/AI/Gemini/controller';\n\nexport async function POST(request: NextRequest) {\n  return generateResponse(request);\n}"],"names":[],"mappings":";;;AACA;;AAEO,eAAe,KAAK,OAAoB;IAC7C,OAAO,CAAA,GAAA,mJAAA,CAAA,mBAAgB,AAAD,EAAE;AAC1B","debugId":null}}]
}